{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Predictive Capabilities of \"Predictive Testing\"\n",
    "## Predicting NCLEX Success for Nursing Students\n",
    "Author: Alexander Stachniak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Boundary Score\n",
    "It is necessary that we develop a numeric score to assist in our analysis of probability boundaries. The \"eye test\" may prove to be inaccurate, especially as we increase the number of points in our dataset.\n",
    "\n",
    "Probability Boundary Score is a new metric which rewards both correct decisions made with high probability and incorrect decisions made with low probability. In other words, these are the situations as we want them to occur:\n",
    "* If the model makes a correct prediction, we want the model to assert that this prediction has a probability as close to 1.00 as possible (near certainty).\n",
    "* If the model makes an incorrect prediction, we want the model to assert that this prediction has a probability as close to 0.50 as possible (split decision).\n",
    "\n",
    "We recognize that prediction can never reach 100% accuracy, and therefore our goal with Probability Boundary Score is to provide a metric whereby we can evaluate classifiers in order to choose one which most fully separates predictions by probability scores. \n",
    "\n",
    "## A Scenario for Illustration\n",
    "We might think of this from the perspective of an educator who is presented with the following scenarios:\n",
    "* Scenario 1: Student A has a 51% chance of passing (no remediation needed); Student B has a 49% chance of passing (remediation needed).\n",
    "* Scenario 2: Student A has a 90% chance of passing (no remediation needed); Student B has a 10% chance of passing (remediation needed).\n",
    "\n",
    "In both cases, our cutoff score for remediation is 0.50, but an educator is much more likely to question the usefulness of the predictive model in Scenario 1. Indeed, for the educator, probability will be conflated with confidence.\n",
    "\n",
    "## What the Scores Mean\n",
    "The minimum and maximum possible values for Probability Boundary Score are 0.00 and 1.00, respectively, providing a close analog to the possible values of standard probability. A score near 0.00 would indicate that a model was either very inaccurate or entirely unable to gain separation between probabilities. As scores increase towards 1.00, the model becomes more accurate and more likely to assert high probabilities for correct predictions.\n",
    "\n",
    "Probability Boundary Score was created as a metric specifically for use in a two-class decision case. To include multiple classes, it would need to be used as a one vs. many calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$pb_{score} = \\frac {\\sum \\left| P_{correct} - B \\right| + \\sum \\bigl( B - \\left| P_{incorrect} - B \\right| \\bigr)} {n} $$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For display of mathematical formulae\n",
    "from IPython.display import Math\n",
    "Math(r'pb_{score} = \\frac {\\sum \\left| P_{correct} - B \\right| + \\sum \\bigl( B - \\left| P_{incorrect} - B \\right| \\bigr)} {n} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* P_correct: the probability assigned to a prediction that turned out to be correct\n",
    "* P_incorrect: the probability assigned to a prediction that turned out to be incorrect\n",
    "* B: the probability boundary (assumed to be 0.50 unless otherwise specified)\n",
    "* n: the total number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Boundary Score and Class Imbalance\n",
    "Note that the equation above could also be modified to produce a Probability Boundary Score that was weighted to account for class imbalance. To do so, we would simply separate the predictions by true class, calculate the Probability Boundary Score for each class, then average the scores together.\n",
    "\n",
    "The overall effect of weighting the Probability Boundary Score will depend on how likely the model is to predict the minority class. Like any metric, this should be used with caution. In a case where the model is extremely unlikely to predict the minority class, each one of those predictions will have an outsized effect on the overall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Probability Boundary as a Class\n",
    "The probability_boundary.py library makes use of Numpy and Matplotlib. Although we could import using \"%run probability_boundary.py\", here we will display the full code for the purpose of review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display plots within Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Caution: use only when certain of results\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load probability_boundary.py\n",
    "\"\"\"\n",
    "Probability Boundary Score\n",
    "\n",
    "@author: zstachniak\n",
    "\"\"\"\n",
    "\n",
    "# Import Statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "class probability_boundary:\n",
    "    '''A class that represents probability scores weighted by true values.'''\n",
    "    \n",
    "    def __init__ (self, y_pred_prob, y_true, boundary=0.50):\n",
    "        '''Function initializes the probability boundary score given \n",
    "        a set of prediction probabilities and the true values. Upon \n",
    "        initialization, function gathers both an unweighted score and \n",
    "        a score that is balanced by class size, in which case the score \n",
    "        is calculated separately for each class and then averaged.\n",
    "\n",
    "        @ Parameters:\n",
    "        ------------------\n",
    "            y_pred_prob: array of probability scores for each class\n",
    "            y_true: array of true class values\n",
    "            boundary: float value between 0.00 and 1.00\n",
    "        '''\n",
    "        # Coerce to ensure values are not imported with unordered indices \n",
    "        # (e.g., a shuffled Pandas series)\n",
    "        self.y_pred_prob = np.asarray(y_pred_prob)\n",
    "        self.y_true = np.asarray(y_true)\n",
    "        \n",
    "        # Preserve boundary value\n",
    "        self.boundary = boundary\n",
    "        \n",
    "        # Unweighted Probability Boundary Score\n",
    "        # Sample Size\n",
    "        n = self.y_true.shape[0]\n",
    "        # Make predictions\n",
    "        y_pred = np.argmax(self.y_pred_prob, axis=1)\n",
    "        # Identify correct predictions\n",
    "        P_correct = self.y_pred_prob[np.argwhere(y_pred == self.y_true)]\n",
    "        # Identify incorrect predictions\n",
    "        P_incorrect = self.y_pred_prob[np.argwhere(y_pred != self.y_true)]\n",
    "        # Caculate Probability Boundary Score\n",
    "        pb_correct = np.sum(np.absolute(P_correct - boundary))\n",
    "        pb_incorrect = np.sum(boundary - np.absolute(P_incorrect - boundary))\n",
    "        self.unweighted = (pb_correct + pb_incorrect) / n\n",
    "        \n",
    "        # Weighted Probability Boundary Score\n",
    "        # Determine class values\n",
    "        classes = np.unique(self.y_true)\n",
    "        # Initialize dictionary for storage and code re-use\n",
    "        pb_dict = {}\n",
    "        # Iterate through classes\n",
    "        for c in classes:\n",
    "            # Separate out classes\n",
    "            pb_dict[c] = {'y_pred_prob': self.y_pred_prob[np.argwhere(self.y_true == c).flatten()],\n",
    "                          'y_true': self.y_true[np.where(self.y_true == c)[0]]\n",
    "                         }\n",
    "            # Sample size\n",
    "            pb_dict[c]['n'] = len(pb_dict[c]['y_true'])\n",
    "            # Make predictions\n",
    "            pb_dict[c]['y_pred'] = np.argmax(pb_dict[c]['y_pred_prob'], axis=1)\n",
    "            # Identify correct predictions\n",
    "            pb_dict[c]['P_correct'] = pb_dict[c]['y_pred_prob'][np.argwhere(pb_dict[c]['y_pred'] == pb_dict[c]['y_true']).flatten()]\n",
    "            # Identify incorrect predictions\n",
    "            pb_dict[c]['P_incorrect'] = pb_dict[c]['y_pred_prob'][np.argwhere(pb_dict[c]['y_pred'] != pb_dict[c]['y_true']).flatten()]\n",
    "            # Calculate Probability Boundary Score\n",
    "            pb_dict[c]['pb_correct'] = np.sum(np.absolute(pb_dict[c]['P_correct'] - boundary))\n",
    "            pb_dict[c]['pb_incorrect'] = np.sum(boundary - np.absolute(pb_dict[c]['P_incorrect'] - boundary))\n",
    "            pb_dict[c]['pb_score'] = (pb_dict[c]['pb_correct'] + pb_dict[c]['pb_incorrect']) / pb_dict[c]['n']\n",
    "        # Average scores for weighted\n",
    "        self.weighted = np.mean([pb_dict[c]['pb_score'] for c in classes])\n",
    "\n",
    "    def __repr__ (self):\n",
    "        'Canonical representation'\n",
    "        return 'probability_boundary({0}, {1}, {2})'.format(self.y_pred_prob, self.y_true, self.boundary)\n",
    "        \n",
    "    def __str__ (self):\n",
    "        'String representation'\n",
    "        return 'Unweighted: {0:.2f} | Weighted {1:.2f}'.format(self.unweighted, self.weighted)\n",
    "            \n",
    "    def weighted_score (self):\n",
    "        'Return the weighted score'\n",
    "        return self.weighted\n",
    "    \n",
    "    def unweighted_score (self):\n",
    "        'Return the unweighted score'\n",
    "        return self.unweighted\n",
    "            \n",
    "    def plot (self, width=14, height=5, subplot=111):\n",
    "        '''A function that displays a plot of probability boundaries.\n",
    "        \n",
    "        @ Parameters:\n",
    "        ------------------\n",
    "            width: width of plot in inches\n",
    "            height: height of plot in inches\n",
    "            subplot: three-digit shorthand for subplot\n",
    "        '''\n",
    "        \n",
    "        # Set default plot size\n",
    "        plt.rcParams['figure.figsize'] = (width, height)\n",
    "\n",
    "        # Number of samples\n",
    "        n = self.y_pred_prob.shape[0]\n",
    "        # x value is the index of array\n",
    "        x = np.arange(n)\n",
    "        # y value is the probability of the element being class 1\n",
    "        y = self.y_pred_prob[:,1]\n",
    "        # Red indicates 0 (Fail) and blue indicates 1 (Pass)\n",
    "        colors = ['red', 'blue']\n",
    "\n",
    "        # Plot probability boundaries\n",
    "        f = plt.figure()\n",
    "        sp = f.add_subplot(subplot)\n",
    "        sp.set_ylim([-0.05, 1.05])\n",
    "        sp.scatter(x, y, c=self.y_true, cmap=ListedColormap(colors), alpha=0.6)\n",
    "        sp.axhline(y=self.boundary, color='r', linestyle='--')\n",
    "        sp.set_title('Probability Boundary (Color = True Class)\\nUnweighted: {0:.2f} | Weighted: {1:.2f}'.format(self.unweighted, self.weighted))\n",
    "        sp.set_xlabel('Observation')\n",
    "        sp.set_ylabel('Class Probability Assigned by Model')\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('df_dict.pickle', 'rb') as file:\n",
    "    df_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'log_pred_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7c8548c5f1e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobability_boundary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'combined'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_pred_prob'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'combined'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'log_pred_prob'"
     ]
    }
   ],
   "source": [
    "test = probability_boundary(df_dict['combined'][7]['log_pred_prob'], df_dict['combined'][7]['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = test.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
